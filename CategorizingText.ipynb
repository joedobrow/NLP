{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "import string\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        if text[i] == '\\n':\n",
    "            if text[i + 1] == '\\n':\n",
    "                return text[(i + 2):]\n",
    "\n",
    "def bow_features(data, common_words):\n",
    "    \n",
    "    bow = []\n",
    "    bow.append(list(data.iloc[:, 0]))\n",
    "    bow.append(list(data.iloc[:, 1]))\n",
    "    \n",
    "    for i in range(len(common_words)):\n",
    "        bow.append(list(np.zeros(len(bow[0]))))\n",
    "    \n",
    "    for i, text in enumerate(bow[0]):\n",
    "        \n",
    "        for word in text:\n",
    "            for word2 in range(len(common_words)):\n",
    "                if word == common_words[word2]:\n",
    "                    bow[word2 + 2][i] += 1\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/Data Science/20_newsgroups'\n",
    "\n",
    "texts = []\n",
    "category = []\n",
    "\n",
    "for filename in os.listdir(root_dir):\n",
    "    if filename != '.DS_Store':\n",
    "        for filename2 in os.listdir(root_dir + '/' + filename):\n",
    "            try:\n",
    "                x = open(root_dir + '/' + filename + '/' + filename2)\n",
    "                raw = x.read()\n",
    "                texts.append(text_cleaner(raw))\n",
    "                category.append(filename)\n",
    "\n",
    "            except:\n",
    "                None\n",
    "\n",
    "new_texts = []\n",
    "for i in range(len(texts)):\n",
    "    \n",
    "    text = texts[i]\n",
    "    text = text.split()\n",
    "    text = ' '.join(text)\n",
    "    text = text.lower()\n",
    "    text = nlp(text)\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_stop\n",
    "                and not token.is_punct\n",
    "               ]\n",
    "    \n",
    "    badwords = ['write', 'article', 'know', 'like', 'think', 'thank']\n",
    "    clean_words = []\n",
    "    for word in allwords:\n",
    "        if (len(word) > 2) and (len(word) < 15) and word.isalpha() and (word not in badwords):\n",
    "            clean_words.append(word)\n",
    "    new_texts.append(clean_words)\n",
    "        \n",
    "data = pd.DataFrame()\n",
    "data['Text'] = new_texts\n",
    "data['Category'] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_common_words = 100\n",
    "common_text = []\n",
    "for text in data.Text:\n",
    "    for word in text:\n",
    "        common_text.append(word)\n",
    "common_words = [item[0] for item in Counter(common_text).most_common(num_common_words)]\n",
    "\n",
    "word_counts = bow_features(data, common_words)\n",
    "for feature in range(num_common_words):\n",
    "    data[common_words[feature]] = word_counts[feature + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:]\n",
    "Y = data['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(num_common_words, 'Words used in Bag of Words')\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Random Forest Score:', rfc.score(X_test, y_test))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN 3 neighbors:', knn.score(X_test, y_test))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=13)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN 13 neighbors:', knn.score(X_test, y_test))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=21)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN 21 neighbors:', knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['Text'] = new_texts\n",
    "data['Category'] = category\n",
    "\n",
    "num_common_words = 1000\n",
    "common_text = []\n",
    "for text in data.Text:\n",
    "    for word in text:\n",
    "        common_text.append(word)\n",
    "common_words = [item[0] for item in Counter(common_text).most_common(num_common_words)]\n",
    "\n",
    "word_counts = bow_features(data, common_words)\n",
    "for feature in range(num_common_words):\n",
    "    data[common_words[feature]] = word_counts[feature + 2]\n",
    "    data[common_words[feature]] = (data[common_words[feature]] - min(data[common_words[feature]]))/max(data[common_words[feature]])\n",
    "    \n",
    "X = data.iloc[:, 2:]\n",
    "Y = data['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(num_common_words, 'Words used in Bag of Words')\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Random Forest Score:', rfc.score(X_test, y_test))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN 3 neighbors:', knn.score(X_test, y_test))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=13)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN 13 neighbors:', knn.score(X_test, y_test))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=21)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN 21 neighbors:', knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['Text'] = new_texts\n",
    "data['Category'] = category\n",
    "\n",
    "num_common_words = 10000\n",
    "common_text = []\n",
    "for text in data.Text:\n",
    "    for word in text:\n",
    "        common_text.append(word)\n",
    "common_words = [item[0] for item in Counter(common_text).most_common(num_common_words)]\n",
    "\n",
    "word_counts = bow_features(data, common_words)\n",
    "for feature in range(num_common_words):\n",
    "    data[common_words[feature]] = word_counts[feature + 2]\n",
    "    data[common_words[feature]] = (data[common_words[feature]] - min(data[common_words[feature]]))/max(data[common_words[feature]])\n",
    "\n",
    "    \n",
    "X = data.iloc[:, 2:]\n",
    "Y = data['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(num_common_words, 'Words used in Bag of Words')\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Random Forest Score:', rfc.score(X_test, y_test))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN 3 neighbors:', knn.score(X_test, y_test))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=13)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN 13 neighbors:', knn.score(X_test, y_test))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=21)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN 21 neighbors:', knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN overfits with larger set of common words. 2000 words is best, and less neighbors seems to be better as well\n",
    "# We would need to normalize and maybe use tfidf instead of BOW to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts_combined = []\n",
    "for i in new_texts:\n",
    "    combined_text = ''\n",
    "    for j in i:\n",
    "        combined_text += j\n",
    "        combined_text += ' '\n",
    "    new_texts_combined.append(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "texts_tfidf = vectorizer.fit_transform(new_texts_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "ntopics=20\n",
    "\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    \n",
    "    words_by_topic=tfidf.T * solution\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    return components\n",
    "\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = components.shape[1]\n",
    "    topwords = []\n",
    "    for column in range(n_topics):\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        topwords.append(chosen)\n",
    "    return(topwords)\n",
    "\n",
    "n_top_words = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd= TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "texts_lsa = lsa.fit_transform(texts_tfidf)\n",
    "\n",
    "components_lsa = word_topic(texts_tfidf, texts_lsa, terms)\n",
    "\n",
    "topwords = []\n",
    "topwords.append(top_words(components_lsa, n_top_words))            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "lda = LDA(n_components=ntopics, \n",
    "          doc_topic_prior=None, # Prior = 1/n_documents\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.1, # Convergence rate.\n",
    "          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning\n",
    "          max_iter=100, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating\n",
    "          random_state=0\n",
    "         )\n",
    "\n",
    "texts_lda = lda.fit_transform(texts_tfidf) \n",
    "\n",
    "components_lda = word_topic(texts_tfidf, texts_lda, terms)\n",
    "\n",
    "topwords.append(top_words(components_lda, n_top_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=200, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "texts_nmf = nmf.fit_transform(texts_tfidf) \n",
    "\n",
    "components_nmf = word_topic(texts_tfidf, texts_nmf, terms)\n",
    "\n",
    "topwords.append(top_words(components_nmf, n_top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "people \t\t good \t\t fbi\n",
      "good \t\t use \t\t koresh\n",
      "time \t\t look \t\t people\n",
      "use \t\t people \t batf\n",
      "say \t\t work \t\t say\n",
      "work \t\t time \t\t start\n",
      "want \t\t mail \t\t child\n",
      "look \t\t file \t\t government\n",
      "right \t\t new \t\t compound\n",
      "new \t\t window \t gas\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "window \t\t new \t\t space\n",
      "file \t\t good \t\t time\n",
      "drive \t\t work \t\t work\n",
      "card \t\t use \t\t people\n",
      "program \t mail \t\t use\n",
      "run \t\t look \t\t thing\n",
      "use \t\t need \t\t year\n",
      "disk \t\t time \t\t good\n",
      "software \t people \t problem\n",
      "driver \t\t window \t say\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "game \t\t use \t\t team\n",
      "team \t\t good \t\t player\n",
      "year \t\t work \t\t win\n",
      "player \t\t look \t\t game\n",
      "play \t\t people \t year\n",
      "win \t\t time \t\t play\n",
      "fan \t\t want \t\t good\n",
      "hockey \t\t problem \t fan\n",
      "baseball \t mail \t\t season\n",
      "season \t\t say \t\t hockey\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "key \t\t people \t key\n",
      "government \t good \t\t chip\n",
      "chip \t\t new \t\t encryption\n",
      "encryption \t look \t\t clipper\n",
      "clipper \t use \t\t government\n",
      "gun \t\t say \t\t escrow\n",
      "law \t\t work \t\t phone\n",
      "phone \t\t time \t\t use\n",
      "public \t\t try \t\t algorithm\n",
      "right \t\t year \t\t encrypt\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "drive \t\t mail \t\t drive\n",
      "car \t\t good \t\t scsi\n",
      "scsi \t\t want \t\t disk\n",
      "ide \t\t people \t ide\n",
      "hard \t\t time \t\t controller\n",
      "buy \t\t look \t\t hard\n",
      "controller \t use \t\t floppy\n",
      "disk \t\t say \t\t problem\n",
      "price \t\t new \t\t work\n",
      "sell \t\t work \t\t use\n",
      "\n",
      "\n",
      "Topic 5:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "key \t\t people \t god\n",
      "chip \t\t use \t\t jesus\n",
      "god \t\t say \t\t believe\n",
      "game \t\t good \t\t bible\n",
      "encryption \t work \t\t people\n",
      "clipper \t time \t\t christian\n",
      "phone \t\t mail \t\t faith\n",
      "drive \t\t problem \t christ\n",
      "jesus \t\t god \t\t sin\n",
      "escrow \t\t window \t say\n",
      "\n",
      "\n",
      "Topic 6:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "car \t\t good \t\t car\n",
      "good \t\t game \t\t engine\n",
      "bike \t\t people \t good\n",
      "look \t\t new \t\t buy\n",
      "new \t\t look \t\t new\n",
      "mail \t\t time \t\t price\n",
      "engine \t\t problem \t drive\n",
      "price \t\t use \t\t look\n",
      "buy \t\t work \t\t sell\n",
      "sell \t\t want \t\t dealer\n",
      "\n",
      "\n",
      "Topic 7:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "card \t\t good \t\t card\n",
      "objective \t use \t\t driver\n",
      "value \t\t look \t\t video\n",
      "monitor \t work \t\t bus\n",
      "video \t\t new \t\t ati\n",
      "moral \t\t time \t\t window\n",
      "driver \t\t drive \t\t graphic\n",
      "morality \t people \t mode\n",
      "frank \t\t problem \t diamond\n",
      "science \t right \t\t work\n",
      "\n",
      "\n",
      "Topic 8:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "drive \t\t giz \t\t objective\n",
      "objective \t good \t\t value\n",
      "value \t\t use \t\t moral\n",
      "file \t\t look \t\t morality\n",
      "moral \t\t time \t\t frank\n",
      "disk \t\t people \t science\n",
      "morality \t new \t\t people\n",
      "science \t window \t mean\n",
      "program \t work \t\t good\n",
      "frank \t\t want \t\t say\n",
      "\n",
      "\n",
      "Topic 9:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "mail \t\t drive \t\t armenian\n",
      "armenian \t window \t turkish\n",
      "israel \t\t use \t\t armenia\n",
      "email \t\t file \t\t turk\n",
      "new \t\t work \t\t genocide\n",
      "book \t\t card \t\t soviet\n",
      "send \t\t problem \t people\n",
      "turkish \t run \t\t turkey\n",
      "address \t mail \t\t muslim\n",
      "list \t\t look \t\t government\n",
      "\n",
      "\n",
      "Topic 10:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "car \t\t god \t\t window\n",
      "window \t\t window \t run\n",
      "armenian \t people \t program\n",
      "key \t\t look \t\t windows\n",
      "turkish \t use \t\t file\n",
      "driver \t\t good \t\t problem\n",
      "run \t\t time \t\t application\n",
      "engine \t\t file \t\t use\n",
      "bike \t\t work \t\t font\n",
      "armenia \t say \t\t driver\n",
      "\n",
      "\n",
      "Topic 11:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "israel \t\t good \t\t israel\n",
      "jews \t\t people \t israeli\n",
      "israeli \t use \t\t jews\n",
      "car \t\t look \t\t arab\n",
      "right \t\t time \t\t jewish\n",
      "jewish \t\t drive \t\t people\n",
      "arab \t\t want \t\t arabs\n",
      "arabs \t\t work \t\t right\n",
      "file \t\t new \t\t state\n",
      "law \t\t problem \t peace\n",
      "\n",
      "\n",
      "Topic 12:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "file \t\t good \t\t gun\n",
      "gun \t\t year \t\t law\n",
      "car \t\t use \t\t people\n",
      "driver \t\t people \t right\n",
      "card \t\t time \t\t government\n",
      "pay \t\t work \t\t weapon\n",
      "format \t\t game \t\t firearm\n",
      "ftp \t\t look \t\t crime\n",
      "game \t\t say \t\t state\n",
      "insurance \t try \t\t criminal\n",
      "\n",
      "\n",
      "Topic 13:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "window \t\t people \t file\n",
      "pay \t\t good \t\t program\n",
      "run \t\t time \t\t image\n",
      "gun \t\t say \t\t format\n",
      "problem \t god \t\t window\n",
      "abortion \t thing \t\t ftp\n",
      "mac \t\t use \t\t use\n",
      "people \t\t want \t\t disk\n",
      "use \t\t need \t\t directory\n",
      "want \t\t work \t\t graphic\n",
      "\n",
      "\n",
      "Topic 14:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "mail \t\t people \t mail\n",
      "window \t\t god \t\t list\n",
      "fbi \t\t say \t\t post\n",
      "sale \t\t right \t\t send\n",
      "price \t\t time \t\t email\n",
      "email \t\t believe \t address\n",
      "offer \t\t government \t look\n",
      "koresh \t\t good \t\t information\n",
      "sell \t\t thing \t\t new\n",
      "list \t\t way \t\t good\n",
      "\n",
      "\n",
      "Topic 15:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "post \t\t work \t\t monitor\n",
      "bike \t\t good \t\t mac\n",
      "insurance \t use \t\t apple\n",
      "driver \t\t time \t\t problem\n",
      "homosexual \t new \t\t use\n",
      "list \t\t window \t modem\n",
      "man \t\t look \t\t work\n",
      "window \t\t mail \t\t computer\n",
      "abortion \t want \t\t color\n",
      "health \t\t people \t card\n",
      "\n",
      "\n",
      "Topic 16:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "pay \t\t people \t abortion\n",
      "insurance \t good \t\t pay\n",
      "abortion \t use \t\t insurance\n",
      "file \t\t time \t\t health\n",
      "health \t\t look \t\t coverage\n",
      "government \t right \t\t want\n",
      "image \t\t work \t\t premium\n",
      "program \t key \t\t dennis\n",
      "space \t\t want \t\t people\n",
      "fbi \t\t thing \t\t private\n",
      "\n",
      "\n",
      "Topic 17:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "bike \t\t good \t\t bike\n",
      "file \t\t look \t\t ride\n",
      "dod \t\t use \t\t dod\n",
      "ride \t\t people \t motorcycle\n",
      "right \t\t new \t\t good\n",
      "monitor \t work \t\t dog\n",
      "motorcycle \t want \t\t buy\n",
      "dog \t\t time \t\t look\n",
      "good \t\t mail \t\t rider\n",
      "rider \t\t say \t\t right\n",
      "\n",
      "\n",
      "Topic 18:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "monitor \t good \t\t homosexual\n",
      "team \t\t say \t\t man\n",
      "new \t\t subscribe \t gay\n",
      "year \t\t people \t people\n",
      "color \t\t mail \t\t clayton\n",
      "homosexual \t use \t\t cramer\n",
      "player \t\t look \t\t sex\n",
      "drive \t\t work \t\t sexual\n",
      "man \t\t time \t\t homosexuality\n",
      "printer \t new \t\t male\n",
      "\n",
      "\n",
      "Topic 19:\n",
      "   ---   ---   ---   \n",
      "LSA\t\t LDA\t\t NNMF\n",
      "game \t\t game \t\t game\n",
      "homosexual \t team \t\t espn\n",
      "car \t\t win \t\t baseball\n",
      "monitor \t year \t\t play\n",
      "man \t\t player \t hockey\n",
      "gay \t\t play \t\t team\n",
      "moral \t\t good \t\t win\n",
      "clayton \t fan \t\t fan\n",
      "cramer \t\t baseball \t watch\n",
      "drive \t\t season \t playoff\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print('   ---   ---   ---   ')\n",
    "    print('LSA\\t\\t LDA\\t\\t NNMF')\n",
    "    for i in range(10):\n",
    "        tab1 = '\\t\\t' if len(topwords[0][topic].index[i]) < 7 else '\\t'\n",
    "        tab2 = '\\t\\t' if len(topwords[1][topic].index[i]) < 6 else '\\t'\n",
    "\n",
    "        print(topwords[0][topic].index[i], tab1, topwords[1][topic].index[i], tab2, topwords[2][topic].index[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth topics:\n",
    "talk.politics.mideast\n",
    "comp.graphics\n",
    "talk.politics.guns\n",
    "talk.politics.misc\n",
    "talk.religion.misc\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\n",
    "misc.forsale\n",
    "rec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.med\n",
    "sci.space\n",
    "soc.religion.christian\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "alt.atheism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA\n",
    "Topic 0: These are common words that don't seem to have a theme. spread: 20\n",
    "1: definitely computer related, there are 5 computer topics so could be any. 5\n",
    "2: sports related. There are 2 sports categories and it seems to overlap between both. 2\n",
    "3: Politics guns? also has encryption so could be politics misc or maybe some bleeding into a computer topic or science electronics. Maybe just sci.crypt? 4.\n",
    "4: electronics, hardware?, comp graphics? 5.\n",
    "5: god mixed in with electronics. 8.\n",
    "6: misc for sale! Very clear 1 topic. 1.\n",
    "7: Weird mix of topics here 10.\n",
    "8: Another mix. 10.\n",
    "9: politics, mideast for sure. 1.\n",
    "10: Mix of cars and mideast politics 2.\n",
    "11: mideast politics. 1.\n",
    "12: cars and software. 3.\n",
    " -- overall a lot of topics seem to be a mix of a few ground truth topics --\n",
    " \n",
    " LDA\n",
    " Awful algorithm, maybe I should try tweaking the parameters\n",
    "\n",
    "NNMF\n",
    "0: Politics misc?\n",
    "1: sci space\n",
    "2: sport hockey\n",
    "3: sci crypt\n",
    "4: sci electronics\n",
    "5: soc religion christian\n",
    "6: rec autos\n",
    "7: comp graphics\n",
    "8: alt atheism or talk religion misc\n",
    "9: talk politics misc\n",
    "10: comp os ms-windows\n",
    "11: talk politics mideast\n",
    "12: talk politics guns\n",
    "13: comp windows x??\n",
    "14: might be a mix of different articles\n",
    "15: mac hardware\n",
    "16: sci med\n",
    "17: rec motorcycles\n",
    "18: homosexuality topics, might be in a couple spots\n",
    "19: rec sport baseball\n",
    "\n",
    "NNMF is blowing the other algoritms out of the water!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
